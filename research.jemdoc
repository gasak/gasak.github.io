# jemdoc: menu{MENU}{index.html}, showsource 

= Research

== Information theory and its interplay with control theory and decision theory.

[img{600}{200}{} figures/twoagents2.png]

Our focus is the design and analysis of communication schemes which can take feedback into account.
Understanding feedback in dynamical and stochastic systems lies at the heart of control theory. 
This motivated us to look at the feedback communication problem from the lens of control theory.
We were able to show that a feedback communication problem can be captured in a control-theoretic framework,
by making some generalizations. To be precise, we look at the causal coding-decoding problem 
where the decision variables could be probability measures and the generalized cost function can capture information gains. 

This opens up a set of results connecting information, control and team-decision theories:

#=== Structural results on coding-decoding problem
#There exists an optimal policy which makes decisions based on a sufficient statistic rather than
#the entire history of observations.

=== Sequential Information Gain Metric 
A metric that can used for sequential decision making problems. An application of this metric is used for 
optimal sensor selection in smart home appliances for minimizing power consumption. 
#*A metric to /sequentially/ measure information gains when looking at a stream of observations* 
#Applications of /sequential information gain/ metric in sensor selection problem is given [ here].
#Mutual information is a well-known metric for measure of information between two correlated sequences. But it lacks in being used as a
#cost\/utility function in control theory problems - the main reason being mutual information cannot be expressed in terms of the decision
#variables if the decision variables or discrete\/gaussian. We propose a more general control-theoretic framework which can capture information gains in the cost\/utility function. 

=== Inverse Optimal Control for Two-Agent team decision problems.
*Understanding the common goal of a team of interacting agents by
observing their behavior.* 
#Given the /policies/ of the agents, we provide the structure of the cost function that is being minimized by
#these agents, among a general class of cost functions. This is the first Inverse Optimal Control result in the control literature for more than
#one agent. We use information-theoretic tools in providing the result.

=== HMMs and Feedback Communication 
*The relation between /stability/ of Hidden Markov Models and /reliability/ of feedback communication systems.* \n 
#Providing conditions when a feedback communication scheme
#is reliable for a given channel is a difficult problem. We provide an
#equivalence between /reliability/ in feedback communication systems to
#/stability/ of the NLF of a Hidden Markov Model. Further, we provide
#necessary and sufficient conditions for reliability of feedback
#communications. Finally, we use Lyapunov theory to show that a feedback
#communication scheme can achieve any positive rate ($0<R<C$) if it can
#reliably transmit a finite number of bits.

#== Shannon Theory

=== Multi-level Unequal Error Protection. 
*Fundamental limits and strategies for unequal error protection for a bitstream of
multiple classes.*\n 
[img{600}{100}{} figures/uep.png]


#Quite often, it is required that a particular part
#of a bitstream requires more protection than the other parts (Eg: packet
#headers vs payload data). For a given data with multiple classes of
#bits, we provide fundamental limits on how good the multi-level
#protection can be, and develop strategies to achieve the same.

=== Optimal Transportation Theory as a solver for Multi-Dim message point communciation.
Fast and Optimal feedback based communication schemes of a multi-dim message point. 
[img{600}{200}{} figures/ott.png]

For example, the figure shows communicatin of a 2D message point on $[0,1]^2$ (e.g. (0.8,0.2)). 


#== Network Coding

#=== Network Coding for Timing Channels *Coding at the nodes when inputs
#and outputs are /timing events/. *\n

#== Timing Channels *When the messages are stored in the packet timings
#rather than their contents.*

#=== Broadcasting Bits Through Queues. Consider a communication system
#where the messages are encoded/tranmistted/decoded from event timings.
#We consider the case where an encoder broadcasts bits through the
#timings to multiple users (some are less noisy than the other). The
#noise is added by a queuing source. We provide an achievable region
#$(R_1,R_2)$ for this broadcast problem with timing i/p and o/p's.
